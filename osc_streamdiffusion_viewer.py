def osc_handler(address, *args, shared_folder=None, signal_emitter=None):
    if len(args) >= 1:
        prompt = str(args[0])
    else:
        print(f"[ERROR] OSCãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ãŒä¸æ­£: {args}")
        return
    
    print(f"[OSCå—ä¿¡] {address}: {prompt}")
    signal_emitter.status_update.emit(f"OSCå—ä¿¡: {prompt[:30]}...")
    signal_emitter.prompt_received.emit(prompt)
    
    thread = threading.Thread(
        target=generate_image,
        args=(prompt, shared_folder, signal_emitter),
        daemon=True,
    )
    thread.start()#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import threading
import time
from pathlib import Path
from datetime import datetime
from queue import Queue
import numpy as np

from PyQt6.QtWidgets import (
    QApplication, QMainWindow, QLabel, QTextEdit, QVBoxLayout,
    QWidget, QPushButton, QHBoxLayout
)
from PyQt6.QtCore import Qt, QTimer, QPropertyAnimation, QEasingCurve, pyqtSignal, QObject, pyqtProperty
from PyQt6.QtGui import QPixmap, QPainter, QImage, QFont
from PIL import Image

from pythonosc import dispatcher
from pythonosc import osc_server

# StreamDiffusioné–¢é€£
try:
    import torch
    from streamdiffusion import StreamDiffusion
    from streamdiffusion.image_utils import postprocess_image
    STREAMDIFFUSION_AVAILABLE = True
except ImportError:
    STREAMDIFFUSION_AVAILABLE = False
    print("[WARNING] StreamDiffusionãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")


# ============================================
# è¨­å®šï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿å¯èƒ½ã«ï¼‰
# ============================================
OSC_IP = "127.0.0.1"
OSC_PORT = 8001
OSC_ADDRESS = "/prompt"

# ãƒ¢ãƒ‡ãƒ«é¸æŠ
MODEL_ID = "runwayml/stable-diffusion-v1-5"  # â˜… æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´
# MODEL_ID = "KBlueLeaf/kohaku-v2.1"  # ã‚¢ãƒ‹ãƒ¡é¢¨ï¼ˆLCM-LoRAå¿…é ˆï¼‰
# MODEL_ID = "stabilityai/sd-turbo"  # é«˜é€Ÿã ãŒå“è³ªä½ã„

USE_LCM_LORA = False  # â˜… ã¾ãšLCM-LoRAãªã—ã§ãƒ†ã‚¹ãƒˆ

IMAGE_WIDTH = 512
IMAGE_HEIGHT = 512

# ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
CURRENT_MODE = "high"  # "high" or "low"
T_INDEX_LIST_HIGH = [0, 12, 25, 37, 45]  # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ï¼ˆ5ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
T_INDEX_LIST_LOW = [0, 24, 45]  # ä½é…å»¶ãƒ¢ãƒ¼ãƒ‰ï¼ˆ3ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
T_INDEX_LIST = T_INDEX_LIST_HIGH  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é«˜å“è³ª

CFG_TYPE = "none"
NUM_INFERENCE_STEPS = 50  # å›ºå®š: StreamDiffusionæ¨å¥¨å€¤
GUIDANCE_SCALE = 0.0

SHARED_FOLDER = Path("./generated_images")

# æ®‹åƒã‚¨ãƒ•ã‚§ã‚¯ãƒˆè¨­å®š
AFTERIMAGE_DURATION = 15000
MAX_AFTERIMAGES = 15
AFTERIMAGE_START_OPACITY = 0.9

# ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼ã‚¨ãƒ•ã‚§ã‚¯ãƒˆè¨­å®š
ENABLE_TYPEWRITER = True
TYPEWRITER_SPEED = 50

# ç”»åƒä¿å­˜è¨­å®š
SAVE_IMAGES = True

# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
DEBUG_MODE = True
USE_DUMMY_MODE = False
USE_STREAMDIFFUSION = False  # â˜… ã¾ãšé€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãƒ†ã‚¹ãƒˆ

# ãƒãƒ«ãƒãƒ¢ãƒ‹ã‚¿ãƒ¼è¨­å®š
USE_FULLSCREEN = False
IMAGE_DISPLAY_MONITOR = 0
PROMPT_DISPLAY_MONITOR = 1


# ============================================
# æ®‹åƒãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¯ãƒ©ã‚¹
# ============================================
class AfterimageLayer:
    def __init__(self, pixmap, start_time):
        self.pixmap = pixmap
        self.start_time = start_time
        self.duration = AFTERIMAGE_DURATION
        self.start_opacity = AFTERIMAGE_START_OPACITY
    
    def get_opacity(self, current_time):
        elapsed = current_time - self.start_time
        if elapsed >= self.duration:
            return 0.0
        progress = elapsed / self.duration
        opacity = self.start_opacity * (1.0 - (progress ** 3))
        return max(0.0, opacity)
    
    def is_expired(self, current_time):
        elapsed = current_time - self.start_time
        return elapsed >= self.duration


class AfterimageLabel(QLabel):
    def __init__(self):
        super().__init__()
        self.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.setStyleSheet("background-color: black;")
        self.setAutoFillBackground(True)
        self.afterimage_layers = []
        self.update_timer = QTimer()
        self.update_timer.timeout.connect(self.update_afterimages)
        self.update_timer.start(16)
    
    def add_image(self, pixmap):
        current_time = time.time() * 1000
        scaled_pixmap = pixmap.scaled(
            self.size(),
            Qt.AspectRatioMode.KeepAspectRatio,
            Qt.TransformationMode.SmoothTransformation
        )
        layer = AfterimageLayer(scaled_pixmap, current_time)
        self.afterimage_layers.append(layer)
        if len(self.afterimage_layers) > MAX_AFTERIMAGES:
            self.afterimage_layers.pop(0)
        self.update()
    
    def update_afterimages(self):
        current_time = time.time() * 1000
        self.afterimage_layers = [
            layer for layer in self.afterimage_layers
            if not layer.is_expired(current_time)
        ]
        self.update()
    
    def paintEvent(self, event):
        painter = QPainter(self)
        painter.fillRect(self.rect(), Qt.GlobalColor.black)
        current_time = time.time() * 1000
        for layer in self.afterimage_layers:
            opacity = layer.get_opacity(current_time)
            if opacity > 0:
                painter.setOpacity(opacity)
                pixmap = layer.pixmap
                x = (self.width() - pixmap.width()) // 2
                y = (self.height() - pixmap.height()) // 2
                painter.drawPixmap(x, y, pixmap)
        painter.end()


class SignalEmitter(QObject):
    image_ready = pyqtSignal(str)
    status_update = pyqtSignal(str)
    prompt_received = pyqtSignal(str)
    mode_changed = pyqtSignal(str)


class PromptWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("å—ä¿¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
        self.setGeometry(920, 100, 500, 600)
        self.setStyleSheet("background-color: black;")
        
        if USE_FULLSCREEN:
            self.setWindowFlags(Qt.WindowType.FramelessWindowHint)
        
        central_widget = QWidget()
        central_widget.setStyleSheet("background-color: black;")
        self.setCentralWidget(central_widget)
        layout = QVBoxLayout(central_widget)
        layout.setContentsMargins(0, 0, 0, 0)
        
        self.typewriter_label = QLabel("")
        self.typewriter_label.setStyleSheet("""
            QLabel {
                color: white;
                font-size: 32px;
                font-family: 'Courier New', monospace;
                background-color: black;
                padding: 20px;
            }
        """)
        self.typewriter_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.typewriter_label.setWordWrap(True)
        layout.addWidget(self.typewriter_label)
        
        self.current_text = ""
        self.target_text = ""
        self.char_index = 0
        self.typewriter_timer = QTimer()
        self.typewriter_timer.timeout.connect(self.type_next_char)
        self.typewriter_speed = TYPEWRITER_SPEED
        
    def add_prompt(self, prompt):
        if ENABLE_TYPEWRITER:
            self.typewriter_timer.stop()
            self.target_text = prompt
            self.char_index = 0
            self.current_text = ""
            self.typewriter_timer.start(self.typewriter_speed)
        else:
            self.typewriter_label.setText(prompt)
    
    def type_next_char(self):
        if self.char_index < len(self.target_text):
            self.current_text += self.target_text[self.char_index]
            self.typewriter_label.setText(self.current_text)
            self.char_index += 1
        else:
            self.typewriter_timer.stop()
    
    def keyPressEvent(self, event):
        if event.key() == Qt.Key.Key_Escape:
            if self.isFullScreen():
                self.showNormal()
            else:
                self.close()
        else:
            super().keyPressEvent(event)


class ImageViewerWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("StreamDiffusion Viewer - ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿å¯¾å¿œ")
        self.setGeometry(100, 100, 800, 700)
        self.setStyleSheet("background-color: black;")
        
        if USE_FULLSCREEN:
            self.setWindowFlags(Qt.WindowType.FramelessWindowHint)
        
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        main_layout = QVBoxLayout(central_widget)
        main_layout.setContentsMargins(0, 0, 0, 0)
        
        self.image_label = AfterimageLabel()
        main_layout.addWidget(self.image_label)
        
        self.status_label = QLabel("å¾…æ©Ÿä¸­...", self)
        self.status_label.setStyleSheet("""
            color: white; 
            background-color: rgba(0, 0, 0, 180);
            padding: 10px;
            font-size: 14px;
        """)
        self.status_label.setAlignment(Qt.AlignmentFlag.AlignLeft | Qt.AlignmentFlag.AlignTop)
        self.status_label.setWordWrap(True)
        
        self.mode_button = QPushButton("ç¾åœ¨: é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ (5ã‚µãƒ³ãƒ—ãƒ«) | ã‚¯ãƒªãƒƒã‚¯ã§ä½é…å»¶ãƒ¢ãƒ¼ãƒ‰ã¸")
        self.mode_button.setStyleSheet("""
            QPushButton {
                background-color: #2196F3;
                color: white;
                font-size: 16px;
                font-weight: bold;
                border-radius: 8px;
                padding: 12px 24px;
                margin: 10px;
            }
            QPushButton:hover {
                background-color: #1976D2;
            }
            QPushButton:pressed {
                background-color: #0D47A1;
            }
        """)
        self.mode_button.clicked.connect(self.toggle_mode)
        main_layout.addWidget(self.mode_button, alignment=Qt.AlignmentFlag.AlignCenter)
        
        self.signal_emitter = SignalEmitter()
        self.signal_emitter.image_ready.connect(self.on_image_ready)
        self.signal_emitter.status_update.connect(self.on_status_update)
        self.signal_emitter.mode_changed.connect(self.on_mode_changed)
        
        self.prompt_window = None
        self.current_mode = "high"
        
    def toggle_mode(self):
        global CURRENT_MODE, T_INDEX_LIST
        
        if self.current_mode == "high":
            self.current_mode = "low"
            CURRENT_MODE = "low"
            T_INDEX_LIST = T_INDEX_LIST_LOW.copy()
            self.mode_button.setText("ç¾åœ¨: ä½é…å»¶ãƒ¢ãƒ¼ãƒ‰ (3ã‚µãƒ³ãƒ—ãƒ«) | ã‚¯ãƒªãƒƒã‚¯ã§é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ã¸")
            self.mode_button.setStyleSheet("""
                QPushButton {
                    background-color: #FF9800;
                    color: white;
                    font-size: 16px;
                    font-weight: bold;
                    border-radius: 8px;
                    padding: 12px 24px;
                    margin: 10px;
                }
                QPushButton:hover {
                    background-color: #F57C00;
                }
                QPushButton:pressed {
                    background-color: #E65100;
                }
            """)
            status_msg = f"ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿: {self.current_mode} (T_INDEX={T_INDEX_LIST})"
        else:
            self.current_mode = "high"
            CURRENT_MODE = "high"
            T_INDEX_LIST = T_INDEX_LIST_HIGH.copy()
            self.mode_button.setText("ç¾åœ¨: é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ (5ã‚µãƒ³ãƒ—ãƒ«) | ã‚¯ãƒªãƒƒã‚¯ã§ä½é…å»¶ãƒ¢ãƒ¼ãƒ‰ã¸")
            self.mode_button.setStyleSheet("""
                QPushButton {
                    background-color: #2196F3;
                    color: white;
                    font-size: 16px;
                    font-weight: bold;
                    border-radius: 8px;
                    padding: 12px 24px;
                    margin: 10px;
                }
                QPushButton:hover {
                    background-color: #1976D2;
                }
                QPushButton:pressed {
                    background-color: #0D47A1;
                }
            """)
            status_msg = f"ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿: {self.current_mode} (T_INDEX={T_INDEX_LIST})"
        
        print(f"[MODE] {status_msg}")
        self.status_label.setText(status_msg)
        self.signal_emitter.status_update.emit(status_msg)
        self.signal_emitter.mode_changed.emit(self.current_mode)
        
        if generator.is_streamdiffusion and generator.is_initialized:
            print(f"[MODE] StreamDiffusionå†åˆæœŸåŒ–ä¸­... (T_INDEX={T_INDEX_LIST})")
            try:
                del generator.pipe
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                generator._initialize_streamdiffusion()
                print("[MODE] å†åˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                print(f"[MODE] å†åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()
    
    def on_mode_changed(self, mode):
        print(f"[SIGNAL] ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´é€šçŸ¥å—ä¿¡: {mode}")
    
    def resizeEvent(self, event):
        super().resizeEvent(event)
        self.status_label.setMaximumWidth(self.width() - 20)
        self.status_label.adjustSize()
    
    def on_status_update(self, status):
        self.status_label.setText(status)
        self.status_label.adjustSize()
        print(f"[STATUS] {status}")
    
    def on_image_ready(self, image_path):
        pixmap = QPixmap(image_path)
        if not pixmap.isNull():
            self.image_label.add_image(pixmap)
    
    def keyPressEvent(self, event):
        if event.key() == Qt.Key.Key_Escape:
            if self.isFullScreen():
                self.showNormal()
            else:
                self.close()
        else:
            super().keyPressEvent(event)


class StreamDiffusionGenerator:
    def __init__(self):
        self.pipe = None
        self.is_initialized = False
        self.generation_times = []
        self.max_history = 10
        self.is_streamdiffusion = False
        self.current_prompt = None
        
    def initialize(self):
        global USE_DUMMY_MODE
        if USE_DUMMY_MODE:
            print("[INFO] ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
            self.is_initialized = True
            return True
        if USE_STREAMDIFFUSION and STREAMDIFFUSION_AVAILABLE:
            return self._initialize_streamdiffusion()
        else:
            print("[INFO] StreamDiffusionãŒç„¡åŠ¹ã¾ãŸã¯ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return self._initialize_normal_pipeline()
    
    def _initialize_streamdiffusion(self):
        try:
            print(f"[INFO] StreamDiffusionåˆæœŸåŒ–ä¸­ï¼ˆ{CURRENT_MODE}ãƒ¢ãƒ¼ãƒ‰ï¼‰...")
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"[INFO] ãƒ‡ãƒã‚¤ã‚¹: {device}")
            if device.type != "cuda":
                print("[WARNING] CUDAãŒä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¾ã™")
                return self._initialize_normal_pipeline()
            
            print(f"[INFO] ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {MODEL_ID}")
            
            from diffusers import StableDiffusionPipeline, AutoencoderTiny, LCMScheduler
            
            pipe = StableDiffusionPipeline.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16,
                safety_checker=None,
            ).to(device)
            
            print("[SUCCESS] ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            
            # LCM-LoRAã‚’é©ç”¨
            if USE_LCM_LORA:
                print("[INFO] LCM-LoRAé©ç”¨ä¸­...")
                try:
                    pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5")
                    pipe.fuse_lora()
                    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)
                    print("[SUCCESS] LCM-LoRAé©ç”¨å®Œäº†")
                    print(f"[INFO] Scheduler: {type(pipe.scheduler).__name__}")
                except Exception as e:
                    print(f"[ERROR] LCM-LoRAé©ç”¨å¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
                    print("[WARNING] LCM-LoRAãªã—ã§ç¶šè¡Œã—ã¾ã™")
            
            print(f"[INFO] T_INDEX_LISTè¨­å®š: {T_INDEX_LIST}")
            print(f"[INFO] CFG_TYPEè¨­å®š: {CFG_TYPE}")
            print(f"[INFO] ç”»åƒã‚µã‚¤ã‚º: {IMAGE_WIDTH}x{IMAGE_HEIGHT}")
            
            self.pipe = StreamDiffusion(
                pipe=pipe,
                t_index_list=T_INDEX_LIST,
                torch_dtype=torch.float16,
                cfg_type=CFG_TYPE,
                width=IMAGE_WIDTH,
                height=IMAGE_HEIGHT,
            )
            
            print("[SUCCESS] StreamDiffusionãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆå®Œäº†")
            
            print("[INFO] TinyVAEèª­ã¿è¾¼ã¿ä¸­...")
            try:
                self.pipe.vae = AutoencoderTiny.from_pretrained(
                    "madebyollin/taesd"
                ).to(device=device, dtype=torch.float16)
                print("[SUCCESS] TinyVAEèª­ã¿è¾¼ã¿å®Œäº†")
            except Exception as e:
                print(f"[WARNING] TinyVAEèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            
            try:
                self.pipe.enable_xformers_memory_efficient_attention()
                print("[SUCCESS] xformersæœ‰åŠ¹åŒ–")
            except Exception as e:
                print(f"[WARNING] xformersç„¡åŠ¹: {e}")
            
            self.pipe.enable_similar_image_filter = False
            
            print("[INFO] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
            try:
                # å…·ä½“çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
                warmup_prompt = "a beautiful anime girl with purple hair, detailed face, high quality"
                print(f"[INFO] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {warmup_prompt}")
                
                self.pipe.prepare(
                    prompt=warmup_prompt,
                    num_inference_steps=NUM_INFERENCE_STEPS,
                    guidance_scale=GUIDANCE_SCALE,
                )
                print("[INFO] prepare()å®Œäº†")
                
                # å®Ÿéš›ã«1æšç”Ÿæˆ
                print("[INFO] ãƒ†ã‚¹ãƒˆç”»åƒç”Ÿæˆä¸­...")
                output = self.pipe()
                print(f"[INFO] ãƒ†ã‚¹ãƒˆç”»åƒç”Ÿæˆå®Œäº†: type={type(output)}")
                
                # ç”»åƒç¢ºèª
                if hasattr(output, 'shape'):
                    print(f"[INFO] å‡ºåŠ›shape: {output.shape}")
                
                self.current_prompt = warmup_prompt
                print("[SUCCESS] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            except Exception as e:
                print(f"[ERROR] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                if DEBUG_MODE:
                    import traceback
                    traceback.print_exc()
                print("[WARNING] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—ã€‚å®Ÿè¡Œã¯ç¶šè¡Œã—ã¾ã™ãŒã€æœ€åˆã®ç”ŸæˆãŒé…ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            
            self.is_streamdiffusion = True
            self.is_initialized = True
            print("[SUCCESS] StreamDiffusionåˆæœŸåŒ–å®Œäº†")
            print(f"[INFO] ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰: {CURRENT_MODE}")
            print(f"[INFO] T_INDEX_LIST: {T_INDEX_LIST}")
            print(f"[INFO] NUM_INFERENCE_STEPS: {NUM_INFERENCE_STEPS}")
            return True
            
        except Exception as e:
            print(f"[ERROR] StreamDiffusionåˆæœŸåŒ–å¤±æ•—: {e}")
            print("[INFO] é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            if DEBUG_MODE:
                import traceback
                traceback.print_exc()
            return self._initialize_normal_pipeline()
    
    def _initialize_normal_pipeline(self):
        try:
            print("[INFO] é€šå¸¸ã®Stable Diffusionãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
            from diffusers import StableDiffusionPipeline
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"[INFO] ãƒ‡ãƒã‚¤ã‚¹: {device}")
            self.pipe = StableDiffusionPipeline.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
                safety_checker=None,
            ).to(device)
            if device.type == "cuda":
                self.pipe.enable_attention_slicing()
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                    print("[INFO] xformers enabled")
                except:
                    pass
            print("[INFO] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
            _ = self.pipe(
                "test",
                num_inference_steps=1,
                guidance_scale=0.0,
            ).images[0]
            self.is_streamdiffusion = False
            self.is_initialized = True
            print("[SUCCESS] é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
            return True
        except Exception as e:
            print(f"[ERROR] åˆæœŸåŒ–å¤±æ•—: {e}")
            if DEBUG_MODE:
                import traceback
                traceback.print_exc()
            return False
    
    def generate(self, prompt):
        print(f"[GENERATE] generate()å‘¼ã³å‡ºã—: prompt='{prompt[:50]}...'")
        print(f"[GENERATE] USE_DUMMY_MODE={USE_DUMMY_MODE}, is_initialized={self.is_initialized}")
        
        if USE_DUMMY_MODE or not self.is_initialized:
            print("[GENERATE] ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™")
            return self._generate_dummy(prompt)
        if len(prompt.strip()) < 3:
            print(f"[SKIP] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒçŸ­ã™ãã¾ã™: '{prompt}'")
            return None
        
        start_time = time.time()
        try:
            if DEBUG_MODE:
                print(f"[DEBUG] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
                print(f"[DEBUG] ãƒ¢ãƒ¼ãƒ‰: {CURRENT_MODE} ({'StreamDiffusion' if self.is_streamdiffusion else 'é€šå¸¸'})")
                print(f"[DEBUG] T_INDEX: {T_INDEX_LIST}")
                print(f"[DEBUG] is_streamdiffusion: {self.is_streamdiffusion}")
            
            if self._contains_japanese(prompt):
                print("[DEBUG] æ—¥æœ¬èªã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚ç¿»è¨³ã‚’è©¦ã¿ã¾ã™...")
                try:
                    from googletrans import Translator
                    translator = Translator()
                    original_prompt = prompt
                    prompt = translator.translate(prompt, src='ja', dest='en').text
                    print(f"[DEBUG] ç¿»è¨³æˆåŠŸ:")
                    print(f"  å…ƒ: {original_prompt[:100]}...")
                    print(f"  è¨³: {prompt[:100]}...")
                except Exception as trans_error:
                    print(f"[WARNING] ç¿»è¨³å¤±æ•—: {trans_error}")
                    print("[WARNING] æ—¥æœ¬èªã®ã¾ã¾å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ï¼ˆå“è³ªä½ä¸‹ã®å¯èƒ½æ€§ï¼‰")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰
            # è¦–è¦šçš„ãªåè©ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            visual_nouns = [
                'woman', 'man', 'person', 'face', 'portrait', 'landscape', 'mountain', 'ocean', 
                'sunset', 'city', 'building', 'car', 'animal', 'cat', 'dog', 'tree', 'flower',
                'sky', 'cloud', 'beach', 'forest', 'river', 'lake', 'bird', 'house', 'girl', 
                'boy', 'child', 'baby', 'street', 'road', 'garden', 'park', 'scene'
            ]
            
            has_visual_content = any(noun in prompt.lower() for noun in visual_nouns)
            is_too_short = len(prompt.split()) < 3
            
            # æŠ½è±¡çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            abstract_keywords = [
                'ææ¡ˆ', 'ãƒ•ã‚©ãƒ¼ãƒ ', 'è€ƒãˆ', 'ã¨ã„ã†', 'æ€ã†', 'ç ”ç©¶', 'å®šç¾©', 'é ˜åŸŸ',
                'suggestion', 'form', 'thinking', 'that', 'this', 'research', 'define', 
                'area', 'concept', 'idea', 'theory', 'important', 'carefully', 'general'
            ]
            has_abstract = any(word in prompt.lower() for word in abstract_keywords)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¸é©åˆ‡ãªå ´åˆ
            if is_too_short or not has_visual_content or has_abstract:
                print("[WARNING] ä¸é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                print(f"[WARNING] å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt[:100]}")
                print(f"[WARNING] åˆ¤å®š: çŸ­ã™ãã‚‹={is_too_short}, è¦–è¦šçš„å†…å®¹ãªã—={not has_visual_content}, æŠ½è±¡çš„={has_abstract}")
                print("[INFO] ãƒ©ãƒ³ãƒ€ãƒ ãªé«˜å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ç½®ãæ›ãˆã¾ã™")
                
                import random
                quality_prompts = [
                    "a beautiful young woman with long flowing hair, golden hour lighting, bokeh background, professional portrait",
                    "majestic mountain landscape with snow peaks, dramatic clouds, sunset colors, 8k photography",
                    "a cute fluffy cat with blue eyes sitting on a windowsill, soft natural lighting",
                    "modern city skyline at night, neon lights reflecting on water, cinematic composition",
                    "serene beach scene at sunset, gentle waves, palm trees silhouette, warm colors",
                    "portrait of an elderly man with wise eyes, dramatic side lighting, black background",
                    "enchanted forest with sunbeams filtering through trees, misty atmosphere, magical mood",
                    "elegant woman in red dress, fashion photography, studio lighting, white background",
                    "powerful waterfall in lush green jungle, long exposure, tropical paradise",
                    "cozy coffee shop interior, warm ambient lighting, bokeh lights, inviting atmosphere"
                ]
                prompt = random.choice(quality_prompts)
                print(f"[INFO] æ–°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ”¿æ²»ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ç³»ã®å ´åˆ
            if any(word in prompt for word in ['é¸æŒ™', 'æ”¿æ²»', 'å›½æ”¿', 'è­°å“¡', 'election', 'politics', 'government']):
                print("[WARNING] ãƒ‹ãƒ¥ãƒ¼ã‚¹/æ”¿æ²»ç³»ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                print("[INFO] é¢¨æ™¯ç”»åƒã«ç½®ãæ›ãˆã¾ã™")
                prompt = "beautiful natural landscape with mountains and pristine lake, golden hour, professional photography"
            
            enhanced_prompt = f"{prompt}, photorealistic, professional photography, 8k uhd, highly detailed face, perfect face, sharp focus, natural lighting, realistic, portrait photography"
            negative_prompt = "low quality, blurry, bad, worst quality, anime, cartoon, painting, illustration, drawing, art, sketch, deformed face, ugly face, bad anatomy, disfigured, mutated"
            
            print(f"[DEBUG] æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {enhanced_prompt[:150]}...")
            
            if self.is_streamdiffusion:
                print(f"[GENERATE] StreamDiffusionã§ç”Ÿæˆé–‹å§‹")
                if self.current_prompt != enhanced_prompt:
                    print(f"[DEBUG] æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§prepare()å®Ÿè¡Œ")
                    print(f"[DEBUG] enhanced_prompt: {enhanced_prompt[:100]}...")
                    self.pipe.prepare(
                        prompt=enhanced_prompt,
                        negative_prompt=negative_prompt,
                        num_inference_steps=NUM_INFERENCE_STEPS,
                        guidance_scale=GUIDANCE_SCALE,
                    )
                    self.current_prompt = enhanced_prompt
                    print("[DEBUG] prepare()å®Œäº†")
                else:
                    print("[DEBUG] åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã®ã§prepare()ã‚¹ã‚­ãƒƒãƒ—")
                
                print("[DEBUG] pipe()å®Ÿè¡Œä¸­...")
                output_image = self.pipe()
                print(f"[DEBUG] pipe()å®Œäº†: type={type(output_image)}")
                
                print("[DEBUG] PILå¤‰æ›é–‹å§‹...")
                output = self._convert_output_to_pil(output_image)
                print(f"[DEBUG] PILå¤‰æ›å®Œäº†: {output.size if output else 'None'}")
            
            else:
                # é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ20ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆï¼‰
                print(f"[GENERATE] é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ç”Ÿæˆé–‹å§‹")
                import random
                
                seed = random.randint(0, 2147483647)
                
                # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’20ã«å›ºå®š
                inference_steps = 20
                guide_scale = 7.5
                
                print(f"[DEBUG] é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: steps={inference_steps}, scale={guide_scale}")
                print(f"[DEBUG] å®Ÿéš›ã®è¨­å®šç¢ºèª: inference_steps={inference_steps}, guidance_scale={guide_scale}")
                
                generator_obj = torch.Generator(device=self.pipe.device).manual_seed(seed)
                print(f"[DEBUG] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–‹å§‹ (seed={seed}, steps={inference_steps})")
                
                result = self.pipe(
                    enhanced_prompt,
                    negative_prompt=negative_prompt,
                    num_inference_steps=inference_steps,
                    guidance_scale=guide_scale,
                    height=IMAGE_HEIGHT,
                    width=IMAGE_WIDTH,
                    generator=generator_obj,
                )
                output = result.images[0]
                print(f"[DEBUG] é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†: {output.size}")
            
            if output is None:
                print("[ERROR] å‡ºåŠ›ç”»åƒãŒNone!")
                return self._generate_dummy("Output is None")
            
            generation_time = time.time() - start_time
            self.generation_times.append(generation_time)
            if len(self.generation_times) > self.max_history:
                self.generation_times.pop(0)
            avg_time = sum(self.generation_times) / len(self.generation_times)
            fps = 1.0 / avg_time if avg_time > 0 else 0
            mode = f"StreamDiffusion({CURRENT_MODE})" if self.is_streamdiffusion else "é€šå¸¸"
            print(f"[PERF] ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’ | å¹³å‡FPS: {fps:.2f} fps | ãƒ¢ãƒ¼ãƒ‰: {mode}")
            print(f"[SUCCESS] ç”»åƒç”ŸæˆæˆåŠŸ: size={output.size}, mode={output.mode}")
            return output
        except Exception as e:
            print(f"[ERROR] ç”»åƒç”Ÿæˆå¤±æ•—: {e}")
            if DEBUG_MODE:
                import traceback
                traceback.print_exc()
            print("[FALLBACK] ãƒ€ãƒŸãƒ¼ç”»åƒã‚’è¿”ã—ã¾ã™")
            return self._generate_dummy(prompt)
    
    def _convert_output_to_pil(self, output_image):
        try:
            if DEBUG_MODE:
                print(f"[DEBUG] å‡ºåŠ›ã‚¿ã‚¤ãƒ—: {type(output_image)}")
                if hasattr(output_image, 'shape'):
                    print(f"[DEBUG] å‡ºåŠ›shape: {output_image.shape}")
                elif hasattr(output_image, 'size'):
                    print(f"[DEBUG] å‡ºåŠ›size: {output_image.size}")
            
            if isinstance(output_image, Image.Image):
                if DEBUG_MODE:
                    print(f"[DEBUG] PIL Image: {output_image.size}, mode: {output_image.mode}")
                return output_image
            
            if isinstance(output_image, torch.Tensor):
                output_np = output_image.detach().cpu().numpy()
            elif isinstance(output_image, np.ndarray):
                output_np = output_image.copy()
            else:
                print(f"[WARNING] æœªå¯¾å¿œã®å‡ºåŠ›å‹: {type(output_image)}")
                return self._generate_dummy(f"Unknown type: {type(output_image)}")
            
            if DEBUG_MODE:
                print(f"[DEBUG] NumPyå¤‰æ›å¾Œshape: {output_np.shape}, dtype: {output_np.dtype}, min: {output_np.min():.3f}, max: {output_np.max():.3f}")
            
            while len(output_np.shape) > 3:
                output_np = output_np.squeeze(0)
                if DEBUG_MODE:
                    print(f"[DEBUG] squeezeå¾Œshape: {output_np.shape}")
            
            # æ­£è¦åŒ–ã®ä¿®æ­£: -1~1ã®ç¯„å›²ã‚’0~255ã«å¤‰æ›
            if output_np.dtype == np.float16 or output_np.dtype == np.float32:
                # å€¤ã®ç¯„å›²ã‚’ç¢ºèª
                min_val = output_np.min()
                max_val = output_np.max()
                
                if DEBUG_MODE:
                    print(f"[DEBUG] æ­£è¦åŒ–å‰: min={min_val:.3f}, max={max_val:.3f}")
                
                # -1~1ã®ç¯„å›²ã®å ´åˆ
                if min_val < 0:
                    # -1~1 ã‚’ 0~1 ã«å¤‰æ›
                    output_np = (output_np + 1.0) / 2.0
                    if DEBUG_MODE:
                        print(f"[DEBUG] -1~1 -> 0~1å¤‰æ›å¾Œ: min={output_np.min():.3f}, max={output_np.max():.3f}")
                
                # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–: 0~1ã®ç¯„å›²ã‚’æœ€å¤§é™ã«æ´»ç”¨
                min_val_normalized = output_np.min()
                max_val_normalized = output_np.max()
                
                if max_val_normalized > min_val_normalized:
                    # å€¤ã‚’0~1ã®å…¨ç¯„å›²ã«ã‚¹ãƒˆãƒ¬ãƒƒãƒ
                    output_np = (output_np - min_val_normalized) / (max_val_normalized - min_val_normalized)
                    if DEBUG_MODE:
                        print(f"[DEBUG] ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–å¾Œ: min={output_np.min():.3f}, max={output_np.max():.3f}")
                
                # 0~1 ã‚’ 0~255 ã«å¤‰æ›
                output_np = (output_np * 255).clip(0, 255).astype(np.uint8)
                
                if DEBUG_MODE:
                    print(f"[DEBUG] 0~255å¤‰æ›å¾Œ: min={output_np.min()}, max={output_np.max()}")
            else:
                output_np = output_np.clip(0, 255).astype(np.uint8)
            
            if len(output_np.shape) == 3:
                if output_np.shape[0] in [1, 3, 4]:
                    output_np = np.transpose(output_np, (1, 2, 0))
                    if DEBUG_MODE:
                        print(f"[DEBUG] transposeå¾Œshape: {output_np.shape}")
            
            if len(output_np.shape) == 2:
                output_np = np.stack([output_np] * 3, axis=-1)
                if DEBUG_MODE:
                    print(f"[DEBUG] ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«->RGB shape: {output_np.shape}")
            
            elif output_np.shape[-1] == 1:
                output_np = np.repeat(output_np, 3, axis=-1)
                if DEBUG_MODE:
                    print(f"[DEBUG] 1ch->RGB shape: {output_np.shape}")
            
            elif output_np.shape[-1] == 4:
                output_np = output_np[:, :, :3]
                if DEBUG_MODE:
                    print(f"[DEBUG] RGBA->RGB shape: {output_np.shape}")
            
            if DEBUG_MODE:
                print(f"[DEBUG] æœ€çµ‚shape: {output_np.shape}, dtype: {output_np.dtype}")
            
            if output_np.shape[-1] == 3 and len(output_np.shape) == 3:
                return Image.fromarray(output_np, mode='RGB')
            else:
                print(f"[WARNING] æƒ³å®šå¤–ã®shape: {output_np.shape}")
                return self._generate_dummy("Shape conversion error")
                
        except Exception as e:
            print(f"[ERROR] ç”»åƒå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            if DEBUG_MODE:
                import traceback
                traceback.print_exc()
            return self._generate_dummy("Conversion error")
    
    def _contains_japanese(self, text):
        for char in text:
            if '\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FFF':
                return True
        return False
    
    def _generate_dummy(self, prompt):
        """ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        # ã‚«ãƒ©ãƒ•ãƒ«ãªã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³èƒŒæ™¯ã‚’ç”Ÿæˆ
        import random
        from PIL import ImageDraw, ImageFont
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ˜ã‚‹ã„è‰²
        colors = [
            (255, 182, 193),  # ãƒ©ã‚¤ãƒˆãƒ”ãƒ³ã‚¯
            (173, 216, 230),  # ãƒ©ã‚¤ãƒˆãƒ–ãƒ«ãƒ¼
            (144, 238, 144),  # ãƒ©ã‚¤ãƒˆã‚°ãƒªãƒ¼ãƒ³
            (255, 218, 185),  # ãƒ”ãƒ¼ãƒ
            (221, 160, 221),  # ãƒ—ãƒ©ãƒ 
            (255, 250, 205),  # ãƒ¬ãƒ¢ãƒ³ã‚·ãƒ•ã‚©ãƒ³
        ]
        color = random.choice(colors)
        
        dummy_image = Image.new('RGB', (IMAGE_WIDTH, IMAGE_HEIGHT), color=color)
        draw = ImageDraw.Draw(dummy_image)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»
        text_lines = [
            "DUMMY MODE",
            f"Prompt: {prompt[:30]}",
            "",
            "Set USE_DUMMY_MODE=False",
            "for real generation"
        ]
        
        y_offset = 50
        for line in text_lines:
            # å½±ã‚’ã¤ã‘ã‚‹
            draw.text((12, y_offset + 2), line, fill=(0, 0, 0))
            draw.text((10, y_offset), line, fill=(255, 255, 255))
            y_offset += 30
        
        print(f"[DUMMY] ãƒ€ãƒŸãƒ¼ç”»åƒç”Ÿæˆ: {prompt[:30]}...")
        return dummy_image


generator = StreamDiffusionGenerator()


def generate_image(prompt, shared_folder, signal_emitter):
    print(f"\n{'='*60}")
    print(f"[TASK] ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯é–‹å§‹")
    print(f"[TASK] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
    print(f"[TASK] å…±æœ‰ãƒ•ã‚©ãƒ«ãƒ€: {shared_folder}")
    print(f"{'='*60}\n")
    
    try:
        signal_emitter.status_update.emit(f"ç”»åƒç”Ÿæˆä¸­ ({CURRENT_MODE}): {prompt[:40]}...")
        
        print("[TASK] generator.generate()å‘¼ã³å‡ºã—...")
        image = generator.generate(prompt)
        print(f"[TASK] generator.generate()å®Œäº†: image={type(image)}")
        
        if image is None:
            print("[TASK] ç”»åƒãŒNoneã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            signal_emitter.status_update.emit(f"ã‚¹ã‚­ãƒƒãƒ—: {prompt[:30]}")
            return
        
        print(f"[TASK] ç”»åƒå–å¾—æˆåŠŸ: size={image.size}, mode={image.mode}")
        
        if SAVE_IMAGES:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")  # ãƒã‚¤ã‚¯ãƒ­ç§’ã‚’è¿½åŠ ã—ã¦é‡è¤‡é˜²æ­¢
            filename = f"generated_{timestamp}.png"
            filepath = shared_folder / filename
            print(f"[TASK] ç”»åƒã‚’ä¿å­˜ä¸­: {filepath}")
            image.save(filepath)
            print(f"[TASK] ä¿å­˜å®Œäº†")
            signal_emitter.status_update.emit(f"ç”Ÿæˆå®Œäº† ({CURRENT_MODE}): {filename}")
            signal_emitter.image_ready.emit(str(filepath))
            print(f"[SUCCESS] ç”»åƒä¿å­˜: {filepath}")
        else:
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                filepath = tmp.name
                print(f"[TASK] ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­: {filepath}")
                image.save(filepath)
            signal_emitter.status_update.emit(f"ç”Ÿæˆå®Œäº† ({CURRENT_MODE})ï¼ˆæœªä¿å­˜ï¼‰")
            signal_emitter.image_ready.emit(str(filepath))
            print(f"[SUCCESS] ç”»åƒç”Ÿæˆå®Œäº†ï¼ˆä¿å­˜ãªã—ï¼‰")
    except Exception as e:
        error_msg = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
        signal_emitter.status_update.emit(error_msg)
        print(f"[ERROR] ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


def osc_handler(address, *args, shared_folder=None, signal_emitter=None):
    if len(args) >= 1:
        prompt = str(args[0])
    else:
        print(f"[ERROR] OSCãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ãŒä¸æ­£: {args}")
        return
    print(f"[OSCå—ä¿¡] {address}: {prompt}")
    signal_emitter.status_update.emit(f"OSCå—ä¿¡: {prompt[:30]}...")
    signal_emitter.prompt_received.emit(prompt)
    thread = threading.Thread(
        target=generate_image,
        args=(prompt, shared_folder, signal_emitter),
        daemon=True,
    )
    thread.start()


def start_osc_server(shared_folder, signal_emitter):
    disp = dispatcher.Dispatcher()
    disp.map(OSC_ADDRESS, lambda addr, *args: osc_handler(
        addr, *args, shared_folder=shared_folder, signal_emitter=signal_emitter
    ))
    try:
        server = osc_server.ThreadingOSCUDPServer((OSC_IP, OSC_PORT), disp)
        print(f"[OSC] ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æˆåŠŸ: {OSC_IP}:{OSC_PORT} (address: {OSC_ADDRESS})")
        server_thread = threading.Thread(target=server.serve_forever, daemon=True)
        server_thread.start()
        return server
    except OSError as e:
        print(f"[ERROR] OSCã‚µãƒ¼ãƒãƒ¼èµ·å‹•å¤±æ•—: ãƒãƒ¼ãƒˆ {OSC_PORT} ãŒæ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™")
        raise


def main():
    SHARED_FOLDER.mkdir(parents=True, exist_ok=True)
    print(f"[INFO] å…±æœ‰ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {SHARED_FOLDER.absolute()}")
    print("[INFO] åˆæœŸåŒ–ä¸­...")
    if not generator.initialize():
        print("[WARNING] åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    
    app = QApplication(sys.argv)
    main_window = ImageViewerWindow()
    prompt_window = PromptWindow()
    screens = app.screens()
    
    if len(screens) > IMAGE_DISPLAY_MONITOR:
        screen = screens[IMAGE_DISPLAY_MONITOR]
        geometry = screen.geometry()
        if USE_FULLSCREEN:
            main_window.setGeometry(geometry)
            main_window.showFullScreen()
        else:
            main_window.setGeometry(geometry.x() + 100, geometry.y() + 100, 800, 700)
            main_window.show()
    else:
        main_window.show()
    
    if len(screens) > PROMPT_DISPLAY_MONITOR:
        screen = screens[PROMPT_DISPLAY_MONITOR]
        geometry = screen.geometry()
        if USE_FULLSCREEN:
            prompt_window.setGeometry(geometry)
            prompt_window.showFullScreen()
        else:
            prompt_window.setGeometry(geometry.x() + 920, geometry.y() + 100, 500, 600)
            prompt_window.show()
    else:
        prompt_window.show()
    
    main_window.signal_emitter.prompt_received.connect(prompt_window.add_prompt)
    main_window.prompt_window = prompt_window
    
    try:
        osc_server_instance = start_osc_server(SHARED_FOLDER, main_window.signal_emitter)
        mode = f"StreamDiffusion({CURRENT_MODE})" if generator.is_streamdiffusion else "é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
        startup_msg = f"""èµ·å‹•å®Œäº† ({mode})
OSC: {OSC_IP}:{OSC_PORT}
ãƒ¢ãƒ‡ãƒ«: {MODEL_ID}
ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰: {CURRENT_MODE}
T_INDEX: {T_INDEX_LIST}
ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {NUM_INFERENCE_STEPS}
äºˆæƒ³FPS: {'15-20fps(é«˜å“è³ª)' if CURRENT_MODE == 'high' else '25-30fps(ä½é…å»¶)'}

ğŸ’¡ ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã§ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿å¯èƒ½"""
        print(f"[INFO] {startup_msg}")
        main_window.signal_emitter.status_update.emit(startup_msg)
        
        print("\n" + "="*60)
        print("StreamDiffusion ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•å®Œäº†")
        print("="*60)
        print("ã€ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã€‘")
        print(f"  - é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰: {len(T_INDEX_LIST_HIGH)}ã‚µãƒ³ãƒ—ãƒ«ã€~15-20fpsã€é¡”ã®å“è³ªUP")
        print(f"  - ä½é…å»¶ãƒ¢ãƒ¼ãƒ‰: {len(T_INDEX_LIST_LOW)}ã‚µãƒ³ãƒ—ãƒ«ã€~25-30fpsã€é«˜é€Ÿç”Ÿæˆ")
        print("  - ç”»é¢ä¸‹éƒ¨ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã§åˆ‡æ›¿")
        print("ã€é‡è¦ã€‘")
        print(f"  - NUM_INFERENCE_STEPSå›ºå®š: {NUM_INFERENCE_STEPS}")
        print(f"  - T_INDEXç¯„å›²: 0ï½{NUM_INFERENCE_STEPS-1}")
        print("="*60 + "\n")
        
    except Exception as error:
        error_msg = f"èµ·å‹•ã‚¨ãƒ©ãƒ¼: {str(error)}"
        print(f"[ERROR] {error_msg}")
        main_window.show()
        prompt_window.show()
        main_window.signal_emitter.status_update.emit(error_msg)
    
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
